# üì• H∆∞·ªõng d·∫´n Download LLaMA 3 Model

## C√°ch 1: Download t·ª´ HuggingFace (Khuy·∫øn ngh·ªã)

### B∆∞·ªõc 1: Truy c·∫≠p HuggingFace

M·ªü link: https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/tree/main

### B∆∞·ªõc 2: Ch·ªçn file ph√π h·ª£p

Ch·ªçn m·ªôt trong c√°c file sau (theo RAM c·ªßa b·∫°n):

| File | Size | RAM c·∫ßn | Ch·∫•t l∆∞·ª£ng | Link |
|------|------|---------|------------|------|
| Meta-Llama-3-8B-Instruct-Q4_K_M.gguf | ~4.9GB | 8GB | ‚≠ê Khuy·∫øn ngh·ªã | [Download](https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf) |
| Meta-Llama-3-8B-Instruct-Q5_K_M.gguf | ~5.7GB | 10GB | Cao | [Download](https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q5_K_M.gguf) |
| Meta-Llama-3-8B-Instruct-Q3_K_M.gguf | ~3.5GB | 6GB | Th·∫•p h∆°n | [Download](https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q3_K_M.gguf) |

### B∆∞·ªõc 3: Download v√† ƒë·∫∑t v√†o th∆∞ m·ª•c models

```bash
# Sau khi download xong, copy file v√†o th∆∞ m·ª•c models/
# V√≠ d·ª•:
# D:\AIAgentAutoTestWeb\models\Meta-Llama-3-8B-Instruct-Q4_K_M.gguf
```

### B∆∞·ªõc 4: C·∫≠p nh·∫≠t config

M·ªü file `config/settings.py` v√† s·ª≠a d√≤ng:

```python
LLAMA_MODEL_PATH = "models/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf"
```

## C√°ch 2: Download b·∫±ng Command Line

### Windows (PowerShell):

```powershell
# Download Q4_K_M (khuy·∫øn ngh·ªã)
Invoke-WebRequest -Uri "https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf" -OutFile "models\Meta-Llama-3-8B-Instruct-Q4_K_M.gguf"
```

### Linux/Mac:

```bash
# Download Q4_K_M (khuy·∫øn ngh·ªã)
wget -P models/ https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf
```

## C√°ch 3: D√πng Ollama (D·ªÖ nh·∫•t)

```bash
# 1. C√†i Ollama t·ª´ https://ollama.ai/download

# 2. Pull model
ollama pull llama3

# 3. Model s·∫Ω ƒë∆∞·ª£c l∆∞u t·∫°i:
# Windows: C:\Users\<username>\.ollama\models\
# Linux/Mac: ~/.ollama/models/

# 4. Copy file .gguf t·ª´ th∆∞ m·ª•c ƒë√≥ v√†o models/
```

## Ki·ªÉm tra sau khi download

```bash
# Ki·ªÉm tra file ƒë√£ t·ªìn t·∫°i
dir models\

# Ch·∫°y test
python main.py https://example.com
```

## L·ªói th∆∞·ªùng g·∫∑p

### L·ªói: "Model not found"

```bash
# Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n
dir models\

# ƒê·∫£m b·∫£o t√™n file trong config/settings.py kh·ªõp v·ªõi file th·ª±c t·∫ø
```

### L·ªói: "Out of memory"

```python
# D√πng model nh·ªè h∆°n (Q3_K_M ho·∫∑c Q4_K_M)
# Ho·∫∑c gi·∫£m context size trong config/settings.py:
LLAMA_N_CTX = 2048  # Thay v√¨ 4096
```

---

**L∆∞u √Ω**: File model r·∫•t l·ªõn (~3-6GB), download c√≥ th·ªÉ m·∫•t 10-30 ph√∫t t√πy t·ªëc ƒë·ªô m·∫°ng.
